# 수익성 평가

## v6/v7 실패 분석

기존 시스템(v6, v7)은 SPY 대비 -17% ~ -29% 언더퍼폼.
핵심 원인: RL 컴포넌트의 과적합과 불충분한 OOS 검증.

## RL 컴포넌트 Cargo Cult ML 평가

현재 RL 구현이 "형식만 ML"인 이유:

| 문제 | 현재 | 필요 수준 |
|------|------|-----------|
| 학습 timesteps | 50K | 500K-2M |
| 가격 전처리 | Z-score (가격 수준 소실) | 수익률 기반 또는 로그 가격 |
| 네트워크 구조 | Flat MLP | LSTM/Transformer + 시계열 임베딩 |
| 보상 함수 | 단순 PnL | 리스크 조정 수익률 (Sharpe 기반) |
| 재학습 주기 | 매주 핫종목 재학습 | 안정적 유니버스, 충분한 학습 기간 |

결론: 현재 RL은 노이즈를 학습하고 있을 가능성이 높다. 가중치 15%에서 0%로 축소하고 재분배.

## 논문 기반 실거래 증거

대부분의 실거래 증거는 AI 트레이딩에 부정적이다.

| 사례 | 결과 |
|------|------|
| Alpha Arena 2025 (LLM 6개) | 4개 손실, 2개만 양(+) |
| Eurekahedge AI Index (14.5년) | AI 연 9.8% vs S&P 500 연 13.7% |
| AIEQ ETF | 연 6.7% vs 벤치마크 11.2% |
| Sentient Technologies | $143M 투자 후 청산 |
| Denny Britz 사례 | 12-18개월 수익 후 전략 디케이 |

## 메타분석 결과

167편 RL 트레이딩 논문 메타분석 (arXiv 2512.10913):

- 알고리즘 선택이 성과에 미치는 영향: **8%**
- 구현 품질(데이터 전처리, 피처 엔지니어링, 비용 모델)이 미치는 영향: **31%**
- 나머지: 시장 레짐, 자산 클래스, 평가 기간 등

시사점: 알고리즘을 바꾸는 것보다 구현 품질을 높이는 것이 4배 효과적.

## RL 가중치 처리 방안

```
변경 전: 팩터 50% + 센티멘트 20% + RL 15% + 테크니컬 15%
변경 후: 팩터 55% + 센티멘트 20% + 테크니컬 25% + RL 0%
```

RL 재도입 조건:
1. 500K+ timesteps 학습
2. PBO(Probability of Backtest Overfitting) < 0.5
3. OOS Sharpe > 0.5 (최소 6개월)
4. Walk-forward 검증 통과

## Lopez de Prado 프레임워크 적용

백테스트 과적합 방지를 위해 필수 도입:

- **PBO (Probability of Backtest Overfitting)**: CSCV 방법으로 전략의 과적합 확률 측정. SSRN 2326253.
- **DSR (Deflated Sharpe Ratio)**: 다중 비교 보정된 Sharpe Ratio. SSRN 2460551.
- **PURGE_DAYS**: 5일에서 21일로 확대 (데이터 누수 방지).
- **거래비용**: 7bps에서 15bps로 상향 (보수적 추정).

기준: PBO > 0.5인 전략은 폐기. DSR이 유의하지 않은 전략은 축소.
